# User Testing Plan: Content Organization by Purpose

## Overview

This document outlines the plan for testing the purpose-based content organization system with real users. The goal is to evaluate the effectiveness of organizing content by purpose rather than by source, and to gather feedback on the user experience.

## Testing Objectives

1. Evaluate the intuitiveness of purpose-based navigation
2. Assess the effectiveness of purpose-specific content presentations
3. Measure user satisfaction with content discovery across purposes
4. Identify any usability issues or areas for improvement
5. Validate the accuracy of purpose classifications

## Participant Recruitment

### Target Participants

We aim to recruit 8-12 participants representing the following user groups:

- Content consumers (4-6 participants)
- Content creators (2-3 participants)
- Content managers (2-3 participants)

### Selection Criteria

Participants should:
- Be familiar with knowledge management systems or content repositories
- Have varying levels of familiarity with the IMAGI-NATION Wiki (from new users to experienced users)
- Represent diverse backgrounds and roles within the organization
- Have different levels of technical expertise

## Testing Methodology

### 1. Pre-Test Questionnaire

Participants will complete a pre-test questionnaire to gather:
- Demographic information
- Experience with content management systems
- Familiarity with the IMAGI-NATION Wiki
- Current content discovery habits

### 2. Task-Based Testing

Participants will complete a series of tasks designed to evaluate different aspects of the purpose-based organization:

#### Navigation Tasks
- Find a specific story about education innovation
- Locate the latest research on a particular theme
- Discover upcoming events related to a specific topic
- Find tools related to a particular implementation context

#### Content Discovery Tasks
- Find related content across different purposes
- Use filters to narrow down content by theme and date
- Search for content with purpose-aware results
- Navigate from the home page to specific purpose hubs

#### Content Evaluation Tasks
- Evaluate the relevance of purpose classifications for specific content items
- Assess the usefulness of purpose-specific presentations
- Rate the effectiveness of related content recommendations

### 3. Think-Aloud Protocol

Participants will be asked to think aloud while completing tasks, verbalizing:
- Their thought process
- Expectations about where to find content
- Reactions to purpose-based organization
- Any confusion or difficulties encountered

### 4. Post-Task Questionnaire

After each task, participants will answer questions about:
- Task completion success (yes/no)
- Task difficulty (1-5 scale)
- Satisfaction with the experience (1-5 scale)
- Any suggestions for improvement

### 5. Post-Test Interview

A semi-structured interview will gather qualitative feedback on:
- Overall impressions of purpose-based organization
- Comparison to previous source-based organization
- Most valuable aspects of the new approach
- Suggestions for improvement
- Any confusion about purpose classifications

## Testing Environment

### Setup

- Testing will be conducted remotely via video conferencing
- Screen sharing will be used to observe participant interactions
- Sessions will be recorded for later analysis (with participant consent)
- The staging environment will be used with real content data

### Tools

- Video conferencing software (Zoom)
- Screen recording software
- Task tracking spreadsheet
- Note-taking template

## Test Scenarios

### Scenario 1: Story Discovery

**Context:** You are interested in learning about personal journeys related to education innovation.

**Tasks:**
1. Navigate to the Stories hub
2. Find stories related to education innovation
3. Filter stories to show only video stories
4. Select and view a story
5. Find related content for that story

### Scenario 2: Research Exploration

**Context:** You need to find research insights about impact assessment.

**Tasks:**
1. Navigate to the Research hub
2. Find research related to impact assessment
3. Filter research to show only synthesis documents
4. Select and view a research document
5. Identify key findings from the research

### Scenario 3: Tool Discovery

**Context:** You are looking for implementation tools for a specific audience.

**Tasks:**
1. Navigate to the Tools hub
2. Find tools for education practitioners
3. Filter tools to show only worksheets
4. Select and view a tool
5. Understand how to use the tool

### Scenario 4: Cross-Purpose Exploration

**Context:** You are interested in a specific theme across all content types.

**Tasks:**
1. Use the search functionality to find content related to "innovation"
2. Identify content from different purposes in the search results
3. Navigate between different purpose hubs to explore the theme
4. Use the home page to discover featured content across purposes
5. Find related content across different purposes

## Data Collection

### Metrics

We will collect the following quantitative metrics:

- Task completion rate (% of tasks completed successfully)
- Time on task (seconds)
- Error rate (number of wrong paths taken)
- Satisfaction ratings (1-5 scale)
- System Usability Scale (SUS) score

### Qualitative Data

We will collect the following qualitative data:

- Think-aloud comments
- Post-task feedback
- Post-test interview responses
- Observed pain points and moments of delight
- Suggestions for improvement

## Analysis Plan

1. Compile quantitative metrics across all participants
2. Identify common patterns in qualitative feedback
3. Map feedback to specific aspects of the purpose-based organization
4. Prioritize issues based on frequency and severity
5. Generate recommendations for improvements

## Timeline

- Week 1: Participant recruitment and scheduling
- Week 2: Conduct user testing sessions (2-3 sessions per day)
- Week 3: Data analysis and report generation
- Week 4: Present findings and recommendations

## Deliverables

1. User testing report with findings and recommendations
2. Raw data from testing sessions (anonymized)
3. Video highlights of key insights
4. Prioritized list of improvements
5. Updated purpose classification rules based on feedback

## Team Roles

- Test Facilitator: Guides participants through the testing process
- Observer: Takes notes and monitors for usability issues
- Technical Support: Assists with any technical issues during testing
- Data Analyst: Compiles and analyzes testing data